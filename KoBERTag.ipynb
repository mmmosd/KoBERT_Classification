{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKz-nNIUuahv",
        "outputId": "a4ecd86f-cc6b-4359-b521-005364f0f8ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.32.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.7.4)\n",
            "Requirement already satisfied: gluonnlp==0.8.0 in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0) (1.23.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: numpy==1.23.1 in /usr/local/lib/python3.10/dist-packages (1.23.1)\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-0tlyucy6/kobert-tokenizer_2369f6cbdcce47fbba8b75af5742c5a8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-0tlyucy6/kobert-tokenizer_2369f6cbdcce47fbba8b75af5742c5a8\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install tqdm pandas\n",
        "!pip install sentencepiece\n",
        "!pip install transformers>=4.8.2\n",
        "!pip install numpy==1.23.1\n",
        "!pip install torch>=1.8.1\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8XpdiEWwuq_u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import gluonnlp as nlp\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DyoX72M_uuDJ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXZzrJekuw_j",
        "outputId": "dcce52d0-df69-43d3-f25f-ed7c373ea39d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UV7kCGCfuz2j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('startup_ideas.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqp68d42u3ey",
        "outputId": "69ddf6e7-0fb3-4a7f-98f5-6a43842fbcb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['스마트 시티 솔루션 개발', [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ],
      "source": [
        "datalabel = data.loc[:, ['Technology','Healthcare & Wellness','Education','Environment & Sustainability','Media & Entertainment','Culture & Arts','Finance & Business','Social Impact & Public Good','Consumer Products & Services','Transportation & Logistics']]\n",
        "\n",
        "data_list = data.apply(lambda row: [row['Idea'], row[1:].tolist()], axis=1)\n",
        "\n",
        "print(data_list[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V43qAhUku-7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2c732e-a26c-4a78-f9a8-e15800bd4f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "500\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)\n",
        "\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X_2SuSdeBH_a"
      },
      "outputs": [],
      "source": [
        "class BERTSentenceTransform:\n",
        "    r\"\"\"BERT style data transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : BERTTokenizer.\n",
        "        Tokenizer for the sentences.\n",
        "    max_seq_length : int.\n",
        "        Maximum sequence length of the sentences.\n",
        "    pad : bool, default True\n",
        "        Whether to pad the sentences to maximum length.\n",
        "    pair : bool, default True\n",
        "        Whether to transform sentences or sentence pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
        "        self._tokenizer = tokenizer\n",
        "        self._max_seq_length = max_seq_length\n",
        "        self._pad = pad\n",
        "        self._pair = pair\n",
        "        self._vocab = vocab\n",
        "\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
        "\n",
        "        The transformation is processed in the following steps:\n",
        "        - tokenize the input sequences\n",
        "        - insert [CLS], [SEP] as necessary\n",
        "        - generate type ids to indicate whether a token belongs to the first\n",
        "        sequence or the second sequence.\n",
        "        - generate valid length\n",
        "\n",
        "        For sequence pairs, the input is a tuple of 2 strings:\n",
        "        text_a, text_b.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'is this jacksonville ?'\n",
        "            text_b: 'no it is not'\n",
        "        Tokenization:\n",
        "            text_a: 'is this jack ##son ##ville ?'\n",
        "            text_b: 'no it is not .'\n",
        "        Processed:\n",
        "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
        "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "            valid_length: 14\n",
        "\n",
        "        For single sequences, the input is a tuple of single string:\n",
        "        text_a.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Tokenization:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Processed:\n",
        "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
        "            type_ids: 0     0   0   0  0     0 0\n",
        "            valid_length: 7\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        line: tuple of str\n",
        "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
        "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
        "            string: (text_a,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: valid length in 'int32', shape (batch_size,)\n",
        "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert to unicode\n",
        "        text_a = line[0]\n",
        "        if self._pair:\n",
        "            assert len(line) == 2\n",
        "            text_b = line[1]\n",
        "\n",
        "        tokens_a = self._tokenizer.tokenize(text_a)\n",
        "        tokens_b = None\n",
        "\n",
        "        if self._pair:\n",
        "            tokens_b = self._tokenizer(text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
        "                                    self._max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > self._max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
        "\n",
        "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
        "        # pre-training and are added to the wordpiece embedding vector\n",
        "        # (and position vector). This is not *strictly* necessary since\n",
        "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        #vocab = self._tokenizer.vocab\n",
        "        vocab = self._vocab\n",
        "        tokens = []\n",
        "        tokens.append(vocab.cls_token)\n",
        "        tokens.extend(tokens_a)\n",
        "        tokens.append(vocab.sep_token)\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens.extend(tokens_b)\n",
        "            tokens.append(vocab.sep_token)\n",
        "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The valid length of sentences. Only real  tokens are attended to.\n",
        "        valid_length = len(input_ids)\n",
        "\n",
        "        if self._pad:\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = self._max_seq_length - valid_length\n",
        "            # use padding tokens for the rest\n",
        "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
        "            segment_ids.extend([0] * padding_length)\n",
        "\n",
        "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
        "            np.array(segment_ids, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "J4S4WvKGvBW9"
      },
      "outputs": [],
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, bert_tokenizer, vocab, max_len, pad, pair):\n",
        "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length = max_len, vocab = vocab, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([txt[0]]) for txt in dataset]\n",
        "        self.labels = torch.tensor([label[1] for label in dataset]).to(device)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        return (self.sentences[i] + (self.labels[i],))\n",
        "\n",
        "    def __len__(self):\n",
        "        return(len(self.labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QEygTDQgvGGH"
      },
      "outputs": [],
      "source": [
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 20\n",
        "max_grad_norm = 1\n",
        "log_interval = 10\n",
        "learning_rate =  5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9nJjQVwXvHX2"
      },
      "outputs": [],
      "source": [
        "data_train = BERTDataset(dataset_train, tokenizer, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, tokenizer, vocab, max_len, True, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aycVvHadvOJs"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Yr3F7jqQvIvc"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, hidden_size = 768, num_classes=10, dr_rate=None, params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmTzCewQvQ7B",
        "outputId": "bdee3ae2-542a-4256-f06c-be2cd462de98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gmL__2oF9hYI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def accuracy(y_true,y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred > 0.5, dtype=float)\n",
        "    y_pred=y_pred.T\n",
        "    y_true=y_true.T\n",
        "    acc_list=[]\n",
        "    for cate in range(0,y_pred.shape[0]):\n",
        "        acc_list.append(accuracy_score(y_pred[cate],y_true[cate]))\n",
        "    return sum(acc_list)/len(acc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPjLlN6mvTq_",
        "outputId": "cfcee3b1-8f45-4159-f47b-ad22cb9dea41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 batch id 1 loss 0.7419866919517517 acc 0.4453125\n",
            "epoch 1 acc 0.8673076923076923\n",
            "epoch 2 batch id 1 loss 0.6935362815856934 acc 0.5484375\n",
            "epoch 2 acc 0.8673076923076923\n",
            "epoch 3 batch id 1 loss 0.6004552841186523 acc 0.7703125\n",
            "epoch 3 acc 0.8673076923076923\n",
            "epoch 4 batch id 1 loss 0.5207034945487976 acc 0.8515625\n",
            "epoch 4 acc 0.8673076923076923\n",
            "epoch 5 batch id 1 loss 0.45216426253318787 acc 0.865625\n",
            "epoch 5 acc 0.8673076923076923\n",
            "epoch 6 batch id 1 loss 0.4061201214790344 acc 0.865625\n",
            "epoch 6 acc 0.8673076923076923\n",
            "epoch 7 batch id 1 loss 0.3689536154270172 acc 0.865625\n",
            "epoch 7 acc 0.8673076923076923\n",
            "epoch 8 batch id 1 loss 0.3369368016719818 acc 0.871875\n",
            "epoch 8 acc 0.8673076923076923\n",
            "epoch 9 batch id 1 loss 0.31327247619628906 acc 0.8734375\n",
            "epoch 9 acc 0.8673076923076923\n",
            "epoch 10 batch id 1 loss 0.29832932353019714 acc 0.8796875\n",
            "epoch 10 acc 0.9096153846153847\n",
            "epoch 11 batch id 1 loss 0.2755594253540039 acc 0.9\n",
            "epoch 11 acc 0.9173076923076924\n",
            "epoch 12 batch id 1 loss 0.2487294226884842 acc 0.9296875\n",
            "epoch 12 acc 0.9384615384615385\n",
            "epoch 13 batch id 1 loss 0.22226004302501678 acc 0.94375\n",
            "epoch 13 acc 0.9538461538461538\n",
            "epoch 14 batch id 1 loss 0.20610933005809784 acc 0.95\n",
            "epoch 14 acc 0.9519230769230769\n",
            "epoch 15 batch id 1 loss 0.1904412806034088 acc 0.9515625\n",
            "epoch 15 acc 0.9576923076923076\n",
            "epoch 16 batch id 1 loss 0.18066422641277313 acc 0.9515625\n",
            "epoch 16 acc 0.9596153846153845\n",
            "epoch 17 batch id 1 loss 0.16444650292396545 acc 0.959375\n",
            "epoch 17 acc 0.9653846153846153\n",
            "epoch 18 batch id 1 loss 0.14756043255329132 acc 0.971875\n",
            "epoch 18 acc 0.9673076923076923\n",
            "epoch 19 batch id 1 loss 0.13164769113063812 acc 0.9734375\n",
            "epoch 19 acc 0.9673076923076923\n",
            "epoch 20 batch id 1 loss 0.12521396577358246 acc 0.971875\n",
            "epoch 20 acc 0.9673076923076922\n"
          ]
        }
      ],
      "source": [
        "train_history = []\n",
        "test_history = []\n",
        "loss_history = []\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length = valid_length\n",
        "        label = label.float().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_acc = accuracy(label.cpu().detach().numpy(), nn.Sigmoid()(out).cpu().detach().numpy())\n",
        "\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc))\n",
        "            train_history.append(train_acc)\n",
        "            loss_history.append(loss.data.cpu().numpy())\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length = valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc = accuracy(label.cpu().detach().numpy(), out.cpu().detach().numpy())\n",
        "    print(\"epoch {} acc {}\".format(e+1, test_acc))\n",
        "    test_history.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8eeq7696h1CW"
      },
      "outputs": [],
      "source": [
        "def predict(predict_sentence):\n",
        "    data = [predict_sentence, [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, tokenizer, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size = batch_size, num_workers = 0)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length = valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        sigmoid = nn.Sigmoid()\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        out = sigmoid(out)\n",
        "\n",
        "        test_eval = []\n",
        "        for logits in out.cpu().detach().numpy()[0]:\n",
        "            test_eval.append(int(logits > 0.5))\n",
        "\n",
        "        result = []\n",
        "        label = ['Technology','Healthcare & Wellness','Education','Environment & Sustainability','Media & Entertainment','Culture & Arts','Finance & Business','Social Impact & Public Good','Consumer Products & Services','Transportation & Logistics']\n",
        "\n",
        "        for i in range(len(label)):\n",
        "          if test_eval[i] == 1:\n",
        "            result.append(label[i])\n",
        "            print(label[i])\n",
        "\n",
        "        return test_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tu4IZnfp0Kom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a1da1d66-0642-4a2b-88ae-0c2872b17fb6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장을 입력해주세요 : 인공지능 챗봇을 이용한 헬스케어 서비스 아이디어\n",
            "Technology\n",
            "Healthcare & Wellness\n",
            "Consumer Products & Services\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 친환경 안경 판매 서비스\n",
            "Environment & Sustainability\n",
            "Consumer Products & Services\n",
            "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 변호사 매칭 서비스\n",
            "Consumer Products & Services\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 노년층을 위한 방송장비 제공 서비스\n",
            "Consumer Products & Services\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 다이어트 교육 플랫폼\n",
            "Education\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 다이어트 가르침 플랫폼\n",
            "Education\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 헬스케어 교육 플랫폼\n",
            "Technology\n",
            "Education\n",
            "[1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 헬스\n",
            "Technology\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 헬스케어 서비스\n",
            "Technology\n",
            "Healthcare & Wellness\n",
            "Consumer Products & Services\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 헬스케어 교육 서비스\n",
            "Technology\n",
            "Consumer Products & Services\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 수륙양용 자동차 배달 서비스\n",
            "Consumer Products & Services\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 수륙양용 자동차 배달\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 배달\n",
            "Technology\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 배달\n",
            "Technology\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 운반\n",
            "Technology\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : AI\n",
            "Technology\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 문화공유커뮤니티\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 문화 공유 커뮤니티\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "문장을 입력해주세요 : 헬스케어서비스\n",
            "Technology\n",
            "Healthcare & Wellness\n",
            "Consumer Products & Services\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 헬스케어 서비스\n",
            "Technology\n",
            "Healthcare & Wellness\n",
            "Consumer Products & Services\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0]\n",
            "문장을 입력해주세요 : 예술품 교환\n",
            "Technology\n",
            "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-85fbf80bd873>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"문장을 입력해주세요 : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "while (True):\n",
        "  sentence = input(\"문장을 입력해주세요 : \")\n",
        "  print(predict(sentence))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}